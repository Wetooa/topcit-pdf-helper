LEARNING GUIDE: Pages 148-152
Generated: 2025-12-05 10:54:49
PDF: 2 - Understanding and Using Data OCR

================================================================================
LEARNING GUIDE
================================================================================

Here is a simplified, easy-to-read learning guide based on the provided text:

---

## Big Data & NoSQL: A Learning Guide

This guide covers key concepts, technologies, and roles related to big data processing and NoSQL databases.

---

### **1. MapReduce**

*   **Definition:** A programming model for parallel, distributed processing of big data using many inexpensive machines.
*   **How it Works:** Processes large datasets in parallel using two main procedures:
    *   **Map:** Processes input data to generate key-value pairs.
    *   **Reduce:** Processes key-value pairs generated by the Map phase to produce the final result.
*   **Purpose:**
    *   Analyzes large-scale data distributed across multiple machines.
    *   Performs batch-based processing.
    *   Ensures data safety by copying, distributing, and storing results to prevent physical device failure.

---

### **2. Visualization Technology**

*   **Purpose:** To gain insights by effectively translating complex numbers, statistics, and large-scale data into easily understandable visual formats. It allows information transfer without users directly analyzing raw data.
*   **Big Data Visualization Methods:**
    *   **Time Visualization:** Shows how data changes over time (e.g., continuous, segmented charts).
    *   **Distribution Visualization:** Shows relationships between a whole and its parts, or ratios (e.g., Pie charts, Treemaps).
    *   **Relationship Visualization:** Displays connections between two or more variables (e.g., Bubble charts, Histograms).
    *   **Comparison Visualization:** Intuitively compares spaces or values (e.g., Heatmaps, Star charts).
    *   **Spatial Visualization:** Maps information onto geographical maps (e.g., including Point of Interest (POI) data).

---

### **3. Classification of Big Data Analytics**

*   **Big Data Analytics:** The process of discovering meaningful patterns from large datasets.
*   **Types of Data Analysis:**
    *   **Descriptive Modeling:**
        *   **Purpose:** To find patterns that describe existing data.
        *   **Techniques:** Association rules, clustering, segmentation, visualization.
    *   **Predictive Modeling:**
        *   **Purpose:** To create a model based on existing data to predict new input data.
        *   **Techniques:** Regression, time series analysis, Neural networks, SVM, Decision Trees.
    *   **Supervised Data Analysis:**
        *   **Characteristic:** A target variable (what you want to predict/classify) is known/determined.
        *   **Techniques:** Neural networks, Case-based reasoning.
    *   **Unsupervised Data Analysis:**
        *   **Characteristic:** No predefined target variable; focuses on finding inherent correlations or similarities between input variables.
        *   **Techniques:** Association rule discovery, K-means clustering.

---

### **4. Main Methods of Big Data Analysis**

*   **Regression Analysis:** A statistical method to predict the probability of an event's occurrence using a linear combination of independent variables.
*   **Decision Tree:** A method that classifies groups or performs predictions by drawing a tree-like chart based on decisions.
*   **Neural Network Analysis:**
    *   Handles problems using parallel, distributed, and probabilistic calculations.
    *   Models digital information as a network of nerve cells, inspired by the human brain.
*   **Text Mining:**
    *   **Purpose:** Extracts and processes useful information from unstructured or semi-structured text data.
    *   **Core Technologies:** Document summarization, document classification, document clustering, and feature extraction.
*   **SNA (Social Network Analysis):**
    *   **Purpose:** Analyzes and visualizes relationships between objects (people, groups, organizations, computers, data) and the structure of their network.
*   **Opinion Mining:**
    *   **Purpose:** Quickly analyzes user information and intelligently infers meaningful insights from large numbers of unstructured reviews (e.g., SNS posts, replies).
    *   **Application:** Useful for corporate marketing and public opinion analysis by identifying trending topics and real-time sentiment.
*   **Natural Language Processing (NLP):**
    *   **Definition:** An artificial intelligence technology that enables computers to understand, create, and analyze human language.
    *   **Process:** Converts human language into a computer-understandable form through mechanical analysis.
    *   **Processing Order:** Preprocessing → Morpheme Analysis → Syntax Analysis → Semantic Analysis → Dialog Analysis.

---

### **5. Data Scientist**

*   **Role:** An expert who collects, organizes, investigates, analyzes, and visualizes data. They provide essential information for decision-making by uncovering value from big data using various platforms and analysis infrastructures.
*   **Key Capabilities:**
    *   **Business Understanding:** Ability to understand business needs and translate them into a business model.
    *   **Data Management:** Expertise in exploring, integrating, and manipulating both structured and unstructured data.
    *   **Data Analysis:** Proficient in predictive analysis (data mining, statistics), cognitive psychology, and using tools like R and visualization techniques.
    *   **Change Management:** Skills in establishing data strategies and effective communication.
    *   **Statistical Analysis Tools:** Experience with tools like R, SAS, SPSS.
    *   **Programming Languages:** Knowledge of languages like C, C++, Java, Ruby, Perl.
    *   **Database Technology (SQL):** Ability to design keys, indexes, queries, normalization, and constraints.
    *   **Distributed Computing:** Understanding of systems like Hadoop (MapReduce, HDFS) and NoSQL databases (Cassandra, BigTable, MongoDB).
    *   **Mathematical Knowledge:** Understanding of matrix operations and numerical analysis.

---

### **6. NoSQL (Not Only SQL)**

#### A) Definition and Characteristics

*   **Definition:** A non-relational, distributed data repository that offers alternative processing methods to traditional tabular relational databases.
*   **Purpose:** Designed for horizontally scalable storage and high-speed writing of unstructured and ultra-high-capacity data across multiple servers.
*   **Key Characteristics:**
    *   **Large Capacity:** Handles petabyte-level data with flexible data structures.
    *   **Flexible Schemas (Schema-less):** Stores data without requiring a predefined schema; uses simplified forms like key-value, graph, or document structures.
    *   **Inexpensive Cluster Configuration:** Supports scale-out (adding more machines), data replication, and distributed storage using affordable commercial hardware.
    *   **Simple Interface:** Uses simple APIs or HTTP calls instead of complex query languages like SQL.
    *   **High Availability:** Automatically distributes data within a cluster and ensures data is available even if a node fails, thanks to replication.
    *   **Integrity Flexibility:** Applications can manage some data integrity aspects, rather than relying solely on the DBMS.
    *   **Elasticity:** Easily expands system scale and performance, distributes I/O load, and avoids downtime during partial system failures.
    *   **Query Capabilities:** Provides specialized query languages and APIs for efficient data search and processing across distributed servers.
    *   **Caching:** Leverages memory-based caching for high-performance response speeds.
    *   **High Scalability:** Allows gradual node increases through partitioning.
    *   **High Performance:** Achieves quick results using memory-based operations, non-blocking writes, and low-complexity algorithms.
    *   **Atomicity:** Ensures each write operation is atomic (indivisible).
    *   **Consistency (Eventual):** Focuses on eventual consistency, meaning data will eventually become consistent, but not necessarily immediately across all nodes. ("Read-Your-Writes" is often sufficient).
    *   **Persistence:** Data is stored persistently on disk.
    *   **Deployment Flexibility:** Automatic data loading on node changes, no special hardware or shared storage requirements.
    *   **Modeling Flexibility:** Conveniently models various data types like key-value pairs, hierarchical data, and graphs.
    *   **Query Flexibility:** Supports various queries, including multiple GETs and range-based key queries.

#### B) BASE Attributes of NoSQL

NoSQL databases often prioritize Availability and Partition Tolerance over strong Consistency, adhering to the **BASE** model:

*   **Basically Available:**
    *   Emphasizes continuous availability, even with multiple failures.
    *   Data copies are stored across multiple locations.
*   **Soft State:**
    *   The state of the system can change over time even without input, due to eventual consistency.
    *   Updates between distributed nodes occur when data eventually propagates to them.
*   **Eventually Consistent:**
    *   Data consistency is maintained optimally over time, even if temporary inconsistencies occur immediately after an update.

*   **Comparison of BASE and ACID Attributes:**

| Attribute           | BASE (NoSQL)                  | ACID (RDBMS)                       |
| :------------------ | :---------------------------- | :--------------------------------- |
| **Application Field** | NoSQL databases               | Relational Database Management Systems |
| **Scope**           | Characteristics of the entire system | Limited to individual transactions |
| **Consistency**     | Weak / Eventual Consistency   | Strong Consistency                 |
| **Main Focus**      | Availability, Performance     | Commit, Strict Data Management     |
| **Efficiency**      | Query Design is crucial       | Table Design is crucial            |

#### C) Storage Methods of NoSQL

NoSQL databases are categorized by their data models:

*   **Key-value based:**
    *   **Description:** The most basic type, offering simple and fast Get, Put, and Delete functions based on unique keys and their associated values.
    *   **Examples:** Dynamo, Redis, MemcacheDB.
*   **Column family based:**
    *   **Description:** Stores data in rows within "column families," which are analogous to tables in relational databases.
    *   **Examples:** Cassandra, HBase, SimpleDB.
*   **Document based:**
    *   **Description:** Stores "documents" (like XML, JSON, or BSON) as the value part in a key-value structure. Each document is self-contained.
    *   **Examples:** MongoDB, CouchDB.
*   **Graph based:**
    *   **Description:** Represents entities as "nodes" and their relationships as "edges" between nodes, ideal for highly interconnected data.
    *   **Examples:** Neo4J, Flock DB.

---

================================================================================
ORIGINAL TEXT (First 5000 chars)
================================================================================

--- Page 148 ---
Database )
@ MapReduce
MapReduce  is a programming  model designed  for the parallel distributed  processing  of big data using inexpensive
machines.  This model can process  large amounts  of data in parallel using a program  composed  of a map procedure
and a reduce  method.  MapReduce  allows the analysis  of large-scale  data by processing  data that has been distributed
and stored in multiple machines.  Basically,  MapReduce  performs  batch-based  processing  and can handle large-scale
data conveniently.  The data of the execution  result is copied, distributed,  and stored safely in consideration  of the
failure of the physical  device.
C) Visualization  technology
This technique  provides  insights  by effectively  transferring  numbers,  statistics,  and valuable  meanings,  by classifying
data for the user’s easy understanding,  and by analyzing  large-scale  data. It allows the exact and effective  transfer  of
information  without  actually  requiring  the user to look at the data.
<Table  719> Big data visualization  method
Technology Description Example
. Peers Shows  the passage  of time.Time visualization . -Continuous,  segmented.
id
Shows the relationship  between  the whole aT
Distribution  visualization  | and the part, and the ratio. ey)
Pie, Treemap
REVENUE  VS. RATING
Shows the relationship  between  two or
Peeuoneile more variables. 2
Bubble  chart, histogram
Comparison Shows spaces and shadows  intuitively.
visualization Heatmap,  Stars
M2 Database  147


--- Page 149 ---
ESSENCE
Shows information  by mapping  it on the
Spatial  visualization map.
Including  PO! data
D) Classification  of big data analytics
Big data analytics  refers to the process  of “discovering  meaningful  patterns  from big data.”
<Table  72> Classification  of data analysis
Classification , ; 5an Modeling Contents Applied technique
Descriptive The primary  purpose  is to find patterns Association  rule, clustering,  database,
modeling that describe  the given data. segmentation,  visualization,  etc.
ues a A model is created based on the given Classification .use Predictive - . Regression,  time series analysis
. data, and is used to predict  new inputmodeling data Neural network
. SVM
Decision  Tree
Supervised  data When the target is determined. Neural network
Presence Case-based  reasoning
of target When there is no target.
variable - The correlation  or similarity  between Association  rule discovery,  market basketUnsupervised  data : .data is analyzed  with the focus on input K-means  clustering
variables.
E) Main methods  of big data analysis
<Table 73> Methods  of big data analysis
Concept Description
+ A statistical  technique  used to predict  the possibility  of an event’s  occurrence  (probabilityee) of occurrence)  using a linear combination  of independent  variables.
+ Amethod  of quantitative  analysis  that classifies  an interested  group into several
Decl  sont  eelaDal  si subgroups  or performs  Prediction  by drawing a decision tree chart.
+ Amethod  of analysis  that handles  a problem  with parallel/distributed/probabilistic
calculation  based on the idea that digital information  is a network  of nerve cells,
rather than a method  of processing  digital information  based on a deterministic  binary
computational  model using the human brain itself as the model.Neural network  analysis
+ A technology  that extracts  and processes  useful information  by applying  natural language
processing  technology  and document  processing  technology  to unstructured/semi-
Text mining structured  data.
+ The core technologies  of text mining  include  document  summarization,  document
classification, document  clustering,  and feature  extraction.
148 TOPCIT  ESSENCE


--- Page 150 ---
Database )
SNA (Social Network  Analysis)+ Asocial  network  refers to the network  between  the components  constituting  a given
society.
+ An analysis  methodology  that analyzes  and visualizes  the relationship  between  objects  -
such as people,  groups, organizations,  computers  and data - and the characteristics  and
structure  of the network.
Opinion  mining+ A technology  that quickly  analyzes  the information  the user wants and intelligently  infers
meaningful  information  from a large number  of unstructured  reviews  such as SNS and
replies.
+ Itis used effectively  for corporate  marketing  policies  or public opinion  analysis  by
extracting  the hot topics of the social network  service  and analyzing  the flow in real time.
Natural  Language  Processing
(NLP)+ An artificial  intelligence  technology  that understands,  creates,  and analyzes  human
language  using computers.
+ The process  of understanding  natural language  by analyzing  human  language
mechanically  to convert  it into a form that can be understood  by computers.
+ Or various  technologies  that express  such a form in language  that humans  can
understand
+ Natural  language  is processed  in the following...
