LEARNING GUIDE: Pages 154-158
Generated: 2025-12-05 10:55:44
PDF: 2 - Understanding and Using Data OCR

================================================================================
LEARNING GUIDE
================================================================================

Here's a simplified, easy-to-read learning guide based on the provided text:

---

# Learning Guide: Databases, AI & Machine Learning Fundamentals

## 1. Database Concepts: Distributed Systems & CAP Theorem

### Key Characteristics of Distributed Databases (CAP Theorem)
When designing distributed databases, there's a trade-off between three key properties:

*   **C - Consistency:** All nodes in the system show the exact same data at the same time. Each user views the same, most up-to-date data.
*   **A - Availability:** The system remains operational and accessible, even if some nodes go down or messages are lost. Users can always read and write data.
*   **P - Partition Tolerance:** The system can continue to operate normally despite network partitions (communication failures between nodes).

**Note:** According to the CAP theorem, a distributed system can only guarantee two out of these three properties simultaneously.

### Database Types based on CAP Combinations

*   **C+A (Consistency + Availability):**
    *   Essential when transactions are required.
    *   Typical of **General RDBMS** (Relational Database Management Systems).
*   **C+P (Consistency + Partition Tolerance):**
    *   Focuses on performance where all nodes must perform well together.
    *   Examples: Google's BigTable, HyperTable, HBase.
*   **A+P (Availability + Partition Tolerance):**
    *   Essential for asynchronous store operations.
    *   Examples: Dynamo, Apache Cassandra, CouchDB, Oracle Coherence.

### NoSQL Systems
*   NoSQL databases are often distributed systems that exhibit characteristics of both **A+P** and **C+P** models, depending on their specific design.

---

## 2. Artificial Intelligence (AI): Overview and Context

### What is AI?
*   **Definition:** Technology that studies how computers can think, learn, and develop by themselves, mimicking human intelligence.
*   **Historical View:** Initially focused on computers imitating human intelligence behaviors.
*   **Modern View:** Broadened to include enabling computers to perform human-like thinking, learning, and self-development across various industries (military, medical, manufacturing, etc.).

### Why AI is Important (Context)
*   **Big Data Challenge:** Traditional database systems struggle with the speed and volume (tens of PBs) of unstructured data generated by multimedia, SNS, sensors, and IoT.
*   AI, coupled with Big Data technologies, provides solutions for analyzing this massive, fast-moving, and varied data.
*   **Ethical Considerations:** Beyond technical understanding, AI development requires considering potential side effects and ethical issues. Principles like the **Asilomar principles** guide responsible AI development.

### Key Keywords (from original text):
*   Artificial intelligence, Turing test, Asilomar principles, machine learning, supervised learning, unsupervised learning, reinforcement learning, deep learning, DNN, RNN, RBN, DBN.

---

## 3. Classifying AI

### Definition & Classification of AI

*   **AI Definition:** Technology that uses computer programs to achieve human-like learning, reasoning, perception, and natural language understanding.

*   **AI Classification:**
    *   **Weak AI / Artificial Narrow Intelligence (ANI):**
        *   Operates only under specific, given conditions.
        *   *Examples:* Google Maps recommendations, autonomous cars, Google Translate, Facebook facial recognition.
    *   **Strong AI / Artificial General Intelligence (AGI):**
        *   AI that can think like humans across a broad range of tasks.
        *   *Examples:* Terminator, advanced secretary robots, factory robots (with human-like reasoning).
    *   **Artificial Super Intelligence (ASI):**
        *   AI that surpasses humans in all areas of intelligence.
        *   *Example:* An AI capable of high-level commands like "Create a new energy source for the next 1,000 years."

---

## 4. History of AI

| Item                 | Description                                                                                                                                              | Detailed Technology / Event                           |
| :------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------- |
| **1950s: Beginnings** | Alan Turing defined methods for testing machine intelligence and the possibility of intelligent/learning machines.                                        | Paper: "Computing Machinery and Intelligence"         |
| **1956: Data-based Analysis System** | Ten scientists held a 6-week workshop at Dartmouth College, marking the start of AI study.                                                                | Dartmouth Workshop                                    |
| **1970s: AI Winter (Dark Age)** | Funding cuts due to AI systems not meeting high expectations.                                                                                            | Perception of unmet expectations                      |
| **1980s: Expert Systems** | Rule-based systems created by manually encoding human expert knowledge. Limitations: expensive to develop/maintain, not useful for small systems. | Rule-based systems                                    |
| **Late 1980s: Second AI Winter** |                                                                                                                                                          |                                                       |
| **1990s: Imitation of Nature** | Shift to manipulating high-level symbols, replacing traditional "logicist" paradigms. Renewed optimism.                                                  | Neural networks, genetic algorithms                   |
| **2010s: Present & Future** | Computers design their own learning models, reuse data for different problems. Fueled by large datasets (Big Data) and improved hardware.              | Machine learning, deep learning, Big Data processing |

---

## 5. Distinguishing AI: Turing Test & Machine Learning

### A) The Turing Test

*   **Definition:** A test of a machine's ability to display intelligent behavior indistinguishable from that of a human.
*   **Principle (Imitation Game):**
    *   A game to determine if a machine possesses intelligence.
    *   A human evaluator communicates with a machine and another human (isolated in different rooms) via text (teletype).
    *   The evaluator asks questions, trying to identify which participant is the machine.
    *   The machine tries to convince the evaluator it is human.
    *   If the machine successfully deceives the evaluator, it is considered to have AI.

### B) Machine Learning (ML)

*   **Definition:**
    *   A program that can solve problems in new situations by learning new knowledge.
    *   A field of study focused on systems that improve their performance through empirical data and interactions with their environment.

### C) Classification of Machine Learning

| Classification            | Description                                                                                                                                                                   | Learning Problem Examples                                                              |
| :------------------------ | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------- |
| **Supervised Learning**   | Uses *labeled* learning data (input X and its target output Y). The goal is to learn a function that maps input to output based on these examples.                                | Classification, Regression, Spam diagnosis, Neural network models, SVM                  |
| **Unsupervised Learning** | Uses *unlabeled* learning data (only input X, no corresponding output Y). The purpose is to understand common characteristics or structures within the input data itself.           | Clustering, Density function estimation, Dimensionality reduction, Feature extraction, K-means |
| **Reinforcement Learning** | (Not fully described in the provided text, but mentioned as a classification in Table 83). Typically involves an agent learning optimal actions through trial and error in an environment, maximizing a reward signal. | (Examples not provided in source text, but generally include game playing, robotics control) |

---

================================================================================
ORIGINAL TEXT (First 5000 chars)
================================================================================

--- Page 154 ---
Database )
+ All nodes should show the same data at the same time.
ons Stency (Each user should always view the same data.)
Availability + Even if some nodes are down, it should not affect the other nodes.
y (All users should always be able to read and write data.)
Pernice  + Even if some messages  are lost, the system  should operate  normally.
(The system  should work properly  in a physically  distributed  network  environment.)
+ An exceptionally  reliable  type in which message  loss can be prevented  even if the system  is down.
C+A + Essential  when a transaction  is required.
+ General  RDBMS.
+ A performance  type in which all nodes must perform  well together.
+ Google's  BigTable,  HyperTable,  HBase.
+ Essential  for asynchronous  store operations.
+ Dynamo,  Apache  Cassandra,  CouchDB,  Oracle  CoherenceC+P
A+P
+ NoSQL  systems  are composed  of distributed  systems  that have the characteristics  of AtP and C+P.
M2 Database 153


--- Page 155 ---
ESSENCE
>> Recent  trends and major issues
Artificial  intelligence  (Al) refers to a technology  that studies how computers  can think, leam, and develop
by themselves  like a human intelligence.  Previously,  Al imitated  human beings as a computer  technology,
but it now is being applied  to most industrial  sectors  including  the military,  aviation,  medical,  manufacturing,
communication,  public administration,  and life services  fields. We are now living in an era in which the ethical
and institutional  issues of Al must be considered,  not to mention  the technical  issues.
>> Learning  objectives
1. To be able to explain the concepts  of Al and machine  leaming.
2. To be able to explain the concept  and algorithm  types of deep learning.
>> Keywords
+ Artificial  intelligence,  Turing test, Asilomar  principles,  machine  learning,  supervised  learning,  unsupervised
learning,  reinforcement  learning,  deep learning,  DNN, RNN, RBN, DBN
154 TOPCIT  ESSENCE


--- Page 156 ---
Database )
* Preview  for practical  business Why Al must understand  both technology
and ethics
Although  interest in data analysis has increased,  the existing  system architecture  and DBMS face
limitations  in terms of processing  speed and performance  when an existing  analysis  system  processes  tens
of PBs of unstructured  data generated  by multimedia,  SNS, sensors,  loT, etc., due to the advancement  of IT
technologies.  As a result, solutions  are being developed  that are suitable  for analyzing  the large volume  of
unstructured  data (variety)  generated  at high speed (velocity).  Technologies  related to big data, which was
still in its infancy  only a few years ago, have developed  rapidly and are now being used directly in our real
life.
The Al field needs to learn the Asilomar  principles  which are created by considering  not only the
understanding  of a given technology  but also the potential  side effects  of applying  that technology.
M2 Database 155


--- Page 157 ---
ESSENCE
01 Overview  of Al
A) Definition  and classification  of Al
© Definition  of Al
Alis a technology  that realizes  human  learning  ability, reasoning  ability, perception  ability,  and natural  language  understanding
ability by using computer  programs.  The dictionary  meaning  of Al is based on a philosophical  concept,  which is created  by
humans  or intelligent  beings or systems.  In the past, Al had the narrow  meaning  of “software  that enables  computers  to imitate
human intelligence  behaviors”.  More recently,  however,  the meaning  of Al has come to include  a technology  that studies  how to
enable  computers  to perform  the kinds of thinking, learning,  and self-development  that can be done with human  intelligence.
@ Classification  of Al
<Table 81> Classification  of Al
Three elements Description Case
Weak Al/Artificial  Narrow  Intelligence:Operable  only under  given conditions.Recommendation  by Google Maps,
Autonomous  Car, Google  Translation,  and
an Facebook
Strong  Al/Artificial  General Terminator,  secretary  robot, factoryintelligence:  AGI Al that can think like humans. robot, etc.
Artificial  Super Intelligence:  ASI Al that surpasses  humans  in all areas.A high-level  command  is possible,  such
as “Create  a new energy  source  that can
be used by people  for the next 1,000
years.”
B) History  of Al
<Table 82> History  of Al
Item Description Detailed  technology
Turing defined  the method  of testing  whether  a machine  can think, the
1950 The beginnings  of Al possibility  of developing  an intelligent  machine,  and a learning  machine,
etc. in his paper “Computing  Machinery  and Intelligence”
1956 Composition  of a data-based Ten scientists  gathered  at Dartmouth  College,  UK and held a 6-week
analysis  system workshop,  which became  the beginnings  of Al study.
1970's Al Winter (Dark Age) Al Winter began as funding  was cut due to the perception  that Al
systems  would never meet the original expectation...
