LEARNING GUIDE: Pages 157-161
Generated: 2025-12-05 10:56:01
PDF: 2 - Understanding and Using Data OCR

================================================================================
LEARNING GUIDE
================================================================================

Here's a simplified, easy-to-read learning guide based on the provided text:

---

# AI & Machine Learning Learning Guide

## 1. Overview of Artificial Intelligence (AI)

### 1.1 Definition of AI
AI is a technology that enables computers to simulate human intelligence. It allows computers to:
*   Learn
*   Reason
*   Perceive
*   Understand natural language
Essentially, AI studies how to make computers think, learn, and self-develop like humans.

### 1.2 Classification of AI

*   **Weak AI (Artificial Narrow Intelligence - ANI):**
    *   Operates only under specific, given conditions.
    *   **Examples:** Google Maps (recommendations), autonomous cars, Google Translate, Facebook features.
*   **Strong AI (Artificial General Intelligence - AGI):**
    *   AI that can think like humans, possessing broad intelligence.
    *   **Examples:** Terminator (fictional), advanced secretary robots, factory robots capable of complex reasoning.
*   **Artificial Super Intelligence (ASI):**
    *   AI that surpasses humans in all cognitive areas.
    *   **Example:** An AI capable of high-level commands like "Create a new energy source for the next 1,000 years."

### 1.3 History of AI (Key Milestones)

*   **1950s:**
    *   **Beginnings of AI:** Alan Turing publishes "Computing Machinery and Intelligence," defining methods to test machine intelligence.
    *   **1956:** Dartmouth College workshop marks the official start of AI study.
*   **1970s:**
    *   **AI Winter (Dark Age):** Funding cuts occur due to unmet expectations for AI systems.
*   **1980s:**
    *   **Expert Systems:** Rule-based systems created by encoding human expert knowledge. Were expensive, difficult to maintain, and often not practical for small applications.
    *   **Second AI Winter:** Occurred in the late 1980s due to limitations of expert systems.
*   **1990s:**
    *   **Imitation of Nature:** New technologies like **Neural Networks** and **Genetic Algorithms** emerge, leading to renewed optimism.
*   **2010s (Present and Future):**
    *   **Machine Learning & Deep Learning:** Computers design their own learning, reuse data, and process large amounts of information (**Big Data**).
    *   Advancements in hardware support these new paradigms.

## 2. Distinguishing AI

### 2.1 The Turing Test

*   **Definition:** A test of a machine's ability to exhibit intelligent behavior that is equivalent to, or indistinguishable from, that of a human.
*   **Principles ("Imitation Game"):**
    1.  A human evaluator communicates with both a human and a machine, both isolated in different rooms (e.g., via teletype).
    2.  The evaluator asks questions to both parties.
    3.  The human tries to confirm the other party is a machine; the machine tries to convince the evaluator it is a real human.
    4.  If the evaluator cannot reliably distinguish the machine from the human, the machine is said to have AI.

## 3. Machine Learning (ML)

### 3.1 Definition of Machine Learning
Machine Learning is a field of AI that allows computer programs to:
*   Solve new problems by **learning new knowledge**.
*   Improve their performance automatically by using **empirical data** based on interactions with the environment.

### 3.2 Classification of Machine Learning

*   **Supervised Learning:**
    *   **Description:** Uses "labeled" learning data, meaning each input has a known, intended output (input/output pairs). The goal is to learn a function that maps inputs to correct outputs.
    *   **Learning Problems:** Classification (e.g., spam detection), Regression (e.g., predicting house prices), Diagnosis.
    *   **Common Models:** Neural networks, Support Vector Machines (SVM).
*   **Unsupervised Learning:**
    *   **Description:** Uses "unlabeled" learning data, meaning only inputs are provided without corresponding outputs. The goal is to discover hidden patterns, structures, or common characteristics within the input data.
    *   **Learning Problems:** Clustering (grouping similar data), Density function estimation, Dimensionality reduction, Feature extraction.
*   **Reinforcement Learning:**
    *   **Description:** An agent learns by performing actions in an environment and receiving rewards for good behavior and penalties for bad behavior (trial and error). It's an intermediate form between supervised and unsupervised learning. The system learns which actions maximize a long-term reward.
    *   **Learning Problems:** Robot control, game playing (e.g., Q-Learning, Deep Q-Networks - DQN), Dynamic Programming.

### 3.3 Supervised Learning Models (Examples)

*   **Neural Network:** An algorithm (often inspired by the human brain) that processes diverse information through interconnected layers, similar to how neurons work.
*   **Hidden Markov Model (HMM):** A probabilistic model used for sequential data. It uses transition probabilities between hidden states and emission probabilities for observed outputs.
*   **Decision Tree:** A tree-like model that makes decisions by sequentially dividing the data space based on features. It can be converted into a set of "if-then" rules.
*   **Multi-Layer Perceptron (MLP):** A type of neural network with multiple layers (input, hidden, output). It uses sigmoid neurons and error correction methods (like backpropagation) and is robust against noise for both discrete and continuous input/output mapping.

### 3.4 Unsupervised Learning Models (Examples)

*   **Clustering:** A technique for finding groups of closely related data points within a dataset, classifying them into subsets called clusters.
*   **K-means Clustering:** An algorithm that groups data by measuring the distance of each data point from the center (centroid) of various clusters. It iteratively adjusts cluster centers until optimal grouping is achieved.
*   **Hierarchical Clustering (HC):** Divides data into a hierarchical structure, creating a tree-like diagram (dendrogram) that visualizes the relationships between clusters. Useful for visualizing multidimensional data.

## 4. Deep Learning

### 4.1 Background and Definition
Deep Learning is an advanced AI technology that enables computers to learn autonomously, similar to humans. It combines aspects of both supervised and unsupervised learning, primarily based on the theory of **Deep Neural Networks (DNNs)**.
*   **Key Idea:** It teaches computers to "think" like humans by processing vast amounts of data through multiple layers of neural networks.

### 4.2 Key Drivers and Context
*   **Big Data Analysis:** The availability of massive volumes and varieties of data ("Big Data") is crucial for deep learning (e.g., Hadoop frameworks).
*   **Overcoming Traditional ML Limitations:** Deep learning helps address issues like:
    *   **Lack of Training Data:** By learning complex features directly from raw data.
    *   **Overfitting:** Through techniques like regularization.
*   **Computational Power:** Requires significant computing resources, leading to:
    *   **GPU Computing:** Leveraging Graphics Processing Units (GPUs) for parallel computation.
    *   **Hardware Development:** Advancements in GPUs and parallel processing.
*   **Applications:** Deep learning is a major driver for advancements in:
    *   Natural Language Processing (NLP)
    *   Pattern analysis (e.g., video, photo recognition)

---

================================================================================
ORIGINAL TEXT (First 5000 chars)
================================================================================

--- Page 157 ---
ESSENCE
01 Overview  of Al
A) Definition  and classification  of Al
© Definition  of Al
Alis a technology  that realizes  human  learning  ability, reasoning  ability, perception  ability,  and natural  language  understanding
ability by using computer  programs.  The dictionary  meaning  of Al is based on a philosophical  concept,  which is created  by
humans  or intelligent  beings or systems.  In the past, Al had the narrow  meaning  of “software  that enables  computers  to imitate
human intelligence  behaviors”.  More recently,  however,  the meaning  of Al has come to include  a technology  that studies  how to
enable  computers  to perform  the kinds of thinking, learning,  and self-development  that can be done with human  intelligence.
@ Classification  of Al
<Table 81> Classification  of Al
Three elements Description Case
Weak Al/Artificial  Narrow  Intelligence:Operable  only under  given conditions.Recommendation  by Google Maps,
Autonomous  Car, Google  Translation,  and
an Facebook
Strong  Al/Artificial  General Terminator,  secretary  robot, factoryintelligence:  AGI Al that can think like humans. robot, etc.
Artificial  Super Intelligence:  ASI Al that surpasses  humans  in all areas.A high-level  command  is possible,  such
as “Create  a new energy  source  that can
be used by people  for the next 1,000
years.”
B) History  of Al
<Table 82> History  of Al
Item Description Detailed  technology
Turing defined  the method  of testing  whether  a machine  can think, the
1950 The beginnings  of Al possibility  of developing  an intelligent  machine,  and a learning  machine,
etc. in his paper “Computing  Machinery  and Intelligence”
1956 Composition  of a data-based Ten scientists  gathered  at Dartmouth  College,  UK and held a 6-week
analysis  system workshop,  which became  the beginnings  of Al study.
1970's Al Winter (Dark Age) Al Winter began as funding  was cut due to the perception  that Al
systems  would never meet the original expectations.
Atule-based  system  created  by manually  entering  the knowledge  of
human experts.
1980's Expert system Although  hundreds  of systems  were built, small-sized  systems  were
not useful, and large-sized  systems  were expensive  to develop  and
maintain,  and difficult  to use.
Second  “Al Winter’  in the late 1980s.
Imitation  of nature Anew  technology  appeared  that focused  on the manipulation  of high-
1990's Neural network,  genetic level symbols  by replacing  the traditional  “logicist”  paradigm.
algorithm Optimism  about  Al
156 TOPCIT  ESSENCE


--- Page 158 ---
Database )
Computers  design by creating  their own learning.
7 Present and future of A\ They reuse data to be learned for other problems.2010's Machine  learning,  deep data th | data i d. and hardleaming Big data that processes  a large amount  of data is spread, and hardware
C) Distinguishing  Al
@ Definition  of the Turing test
+ Atest of a machine's  ability to exhibit  intelligent  behavior  equivalent  to, or indistinguishable  from, that of a human.
+ In a given problem-solving  thinking  situation,  if the evaluator  cannot reliably tell the machine  from the human,  the
machine  is said to have thought  in the same way as a human being.
@ Principles  of the Turing  test
+ The Turing test is a kind of game designed  to test whether  a machine  can have intelligence.
+ In the ‘Imitation  Game’, people and machines  are isolated  in different  rooms and communicate  by teletype.
+ The people try to confirm  that the other party is a machine  by asking it questions,  and the machine  tries to lead
the people to believe  that it is a real human through  its answers.
+ If the machine  wins the game, the machine  can be said to have Al.
D) Machine  learning
@ Definition  of machine  learning
+ Aprogram  that can solve problems  in a new situation  while learning  new knowledge.
+ Science and technology  study a system that improves  its performance  by itself using empirical  data based on
interactions  with the environment.
@ Classification  of machine  learning
<Table 83> Supervised  learning,  unsupervised  learning,  reinforcement  learning
Classification Description Learning  problem
A machine  learning  method  that uses learning  data, which describes  theintended  result. Classification,  regression,
au Learning a function that maps the input from learning examples,  which are Sion diagnosis,
composed  of a pair of |/O, and the output. oul network model. SVM
A learning  method  that presents  input x and its target output  y. ,
A machine  learning  method  that uses learning  data, which does not
describe  the intended  result.
Applicable to the clust blem  bi \ ti be built . . . pplicable to the clustering  problem because learning  data can be bui Clustering,  density  function
Unsupervised easily ia though pladipaeats  Poor. i h estimation, dimensionallearning For example,  input is given, but there is no corresponding  output. The reduction,  fea...
