LEARNING GUIDE: Pages 52-56
Generated: 2025-12-07 00:58:03
PDF: 3 - Overview of System Architecture OCR

================================================================================
LEARNING GUIDE
================================================================================

This learning guide summarizes the essential concepts from the provided text (Pages 52-56) regarding System Architecture, focusing on memory management and process scheduling.

---

## Learning Guide: System Architecture Essentials

### 1. Memory Fragmentation

**Fragmentation** refers to wasted memory space that cannot be used by processes.

*   **External Fragmentation:**
    *   **What it is:** Wasted memory space *between* allocated memory blocks. Occurs when there's enough total free space, but it's not contiguous (broken into small, unusable pieces).
    *   **Cause:** Variable-size memory allocation techniques (e.g., first-fit, best-fit, worst-fit).
    *   **Example:** A process requests 64,450 bytes, and 64,500 bytes are available. After allocation, 50 bytes remain *between* used blocks.

*   **Internal Fragmentation:**
    *   **What it is:** Wasted memory space *within* an allocated memory block. The allocated block is slightly larger than the requested size.
    *   **Cause:** Fixed-size memory partitioning, where a process is given a block larger than it needs.
    *   **Example:** A fixed partition is 65,000 bytes, but a process only needs 64,450 bytes. 50 bytes are wasted *inside* that allocated partition.

#### 1.1 Solving Fragmentation

*   **Compaction Technique:**
    *   **Purpose:** Merges small, available memory blocks into a larger contiguous block.
    *   **Target:** Primarily removes **external fragmentation** in variable-size memory systems.
    *   **Drawbacks:** Complex to implement, causes high overhead, and is time-consuming (not widely used).

*   **Coalescing Technique:**
    *   **Purpose:** Merges adjacent empty (unused) spaces in the free space list.
    *   **Goal:** To create larger available memory spaces and prevent many small, unusable spaces from accumulating.

---

### 2. Operating System Scheduling

**Scheduling** is how the Operating System (OS) efficiently allocates processes to the Central Processing Unit (CPU) in multi-programming environments.

#### 2.1 Purpose of Scheduling

*   Ensure fairness to processes.
*   Maximize throughput (processes completed per unit time).
*   Minimize response time (time until first response).
*   Achieve predictable execution time.
*   Prevent system overload.
*   Balance resource utilization.
*   Prevent indefinite process execution (starvation).
*   Implement priority schemes for critical tasks.

#### 2.2 Types of Scheduling

*   **Preemptive Scheduling:**
    *   **Concept:** A running process can be interrupted and have the CPU taken away by another, higher-priority process or after a time slice.
    *   **Characteristic:** Allows for better responsiveness and fairer resource distribution.

*   **Non-preemptive Scheduling:**
    *   **Concept:** Once a CPU resource is allocated to a process, it cannot be taken away until the process voluntarily releases it (e.g., completes its task or waits for I/O).
    *   **Characteristic:** Simple to implement but can lead to longer waiting times for other processes.

#### 2.3 Scheduling Algorithms

| Algorithm | Type        | Characteristics                                                                                                        |
| :-------- | :---------- | :--------------------------------------------------------------------------------------------------------------------- |
| **FIFO**  | Non-preemptive | Simplest: Processes are allocated CPU in the order they request it (First In, First Out). Simple, fair, but not ideal for interactive systems. |
| **Priority**| Non-preemptive | Each process is assigned a priority, and the CPU is allocated to the highest-priority process. Priorities can be fixed, variable, or purchased. |
| **SJF**   | Non-preemptive | **Shortest Job First:** Allocates CPU to the process with the shortest *expected* execution time. Advantageous for short jobs. |
| **SRT**   | Non-preemptive | **Shortest Remaining Time:** Allocates CPU to the process with the shortest *expected remaining* execution time. Can be seen as a preemptive SJF (though listed as non-preemptive in the source text). Theoretically minimizes waiting time. |
| **R-R**   | Preemptive  | **Round Robin:** Similar to FIFO, but each process gets a fixed "time slice" on the CPU. After its time slice, it's preempted and put back in the queue. Effective for Time-Sharing Systems (TSS). Frequent context switching if time slice is too short. |
| **Deadline**| Non-preemptive | Ensures each process completes within a specified time limit. High overhead due to continuous deadline calculations. |
| **HRN**   | Non-preemptive | **Highest Response-ratio Next:** Addresses SJF's issue of favoring short jobs by prioritizing jobs based on their response ratio: `(Waiting time + Execution time) / Execution time`. |
| **MLQ**   | Preemptive  | **Multi-Level Queue:** Divides processes into multiple queues, each using its own scheduling algorithm. Processes different jobs via time slices within each queue. |
| **MFO**   | Preemptive  | **Multi-Level Feedback Queue:** Processes jobs through several queues (feedback queues) where processes can move between queues based on their behavior (e.g., CPU-bound vs. I/O-bound). Improves CPU and I/O device efficiency. |

---

### 3. Virtual Memory Unit

**Virtual Memory** is a technique that allows an OS to use a small amount of main memory (RAM) and a large amount of auxiliary memory (disk) to appear as a much larger, continuous main memory space to processes. It is a logical concept, not a physical device.

#### 3.1 Implementing Virtual Memory

*   **Virtual Address:** The address processes refer to (larger than physical memory).
*   **Physical Address:** The actual address in the main memory.
*   **Memory Management Unit (MMU):** A hardware component responsible for quickly converting (mapping) virtual addresses to physical addresses whenever a process accesses a virtual address.

Virtual memory implementation is primarily divided into two techniques based on how memory blocks are configured:

*   **Paging Technique:**
    *   **Concept:** Divides main memory into fixed-size blocks called **frames**. Divides a process's virtual memory into fixed-size blocks called **pages**.
    *   **Process:** Loads pages into available frames in main memory.

*   **Segmentation Technique:**
    *   **Concept:** Divides a process's virtual memory into logical units of various (variable) sizes called **segments**.
    *   **Process:** Loads these segments into available main memory space.

*   **Hybrid Systems:** Some systems use a combination of both paging and segmentation.

#### 3.2 Page Replacement Techniques

When all frames in main memory are full and a new page needs to be loaded from auxiliary memory, an existing page must be replaced. The choice of which page to replace significantly affects system efficiency and performance.

*   **Optimal Technique:**
    *   **Rule:** Replaces the page that will **not be used for the longest time** in the future.
    *   **Characteristic:** Provides the absolute minimum page fault rate.
    *   **Feasibility:** Not realistic or implementable because it requires predicting future page access patterns, which is impossible.

*   **First In First Out (FIFO) Technique:**
    *   **Rule:** Replaces the page that was **loaded into main memory first**.
    *   **Mechanism:** Tracks the loading order of pages.

*   **Least Recently Used (LRU) Technique:**
    *   **Rule:** Replaces the page that has been **unused for the longest time**.
    *   **Mechanism:** Tracks the last time each page was accessed.

*   **Least Frequently Used (LFU) Technique:**
    *   **Rule:** Replaces the page that has been **used the least number of times** (or least intensively) since it was loaded.
    *   **Mechanism:** Tracks the utilization frequency of each page.

*   **Not Used Recently (NUR) Technique:**
    *   **Rule:** Replaces a page that has **not been used recently**, based on the assumption that a recently unused page is less likely to be used soon.
    *   **Mechanism:** Uses reference bits and dirty bits to approximate LRU behavior without high overhead.

---

================================================================================
ORIGINAL TEXT (First 5000 chars)
================================================================================

--- Page 52 ---
Overview  of System Architecture )
recollection  can result in memory  space that is wasted without  being used. Using the first-fit, best-fit,  and worst-
fit techniques  to allocate  the memory  unit can cause external  fragmentation,  as shown in [Figure 22]. Let us assume
that a process  requests  64.450 bytes when there are 64,500 bytes of available space. In that case, 50 bytes of
available  space remains  after allocating  the requested  space. If the memory  is partitioned  into a fixed size to provide
multiple  fixed-sized  available  spaces to processes,  it may be slightly larger than the required space. The remaining
space is called the internal  fragmentation,  as shown in [Figure 23].
Operating  System
Request  for available Process 1
space of 64,450  bytes
Request  for available
space of 64,450  bytes
Process  50
[Figure  22] Case of External  Fragmentation
Fixed Partition Fixed Partition
65,000 Bytes 65,000  Bytes
facia mre elmic)  on)
(30,000 Bytes) ee ee ey
; , Section  in Use Section  in Use
Available  Section
Request aa
65,000 Bytes
Unused  Section
Use
65,000 Bytes J
Unused  Section Internal Fragmentation
(30,000 Bytes)
65,000 Bytes 65,000  Bytes
Section in Use Section in Use
100,000  Bytes 100,000  Bytes
Request Fragmentation  after
Allocation
[Figure 23] Case of Internal  Fragmentation
M3 Overview  of System  Architecture  51


--- Page 53 ---
ESSENCE
© Solving  the fragmentation  problem
* Compaction  technique:
The compaction  technique  merges small-sized  available  memory.  It is used to remove  the external  fragmentation
that generated  in a variable-size  memory  partition  system, as shown in [Figure 24]. The technique  is not widely
used because  it is complex  to implement,  and it causes  overload  as it requires  a long time.
Operating  System Operating  System
Compression
In Use > In Use
20K Unused Compression In Use
In Use In Use
10KB Unused Compression
In Use 80KB Unused
50KB Unused
[Figure  24] Solving  Fragmentation  Problem  through  Compaction  Technique
* Coalescing  technique:
The coalescing  technique  merges  spaces  with adjacent  addresses  in the unused empty space list in order to create
a larger space and to prevent  multiple  small spaces,  as shown in [Figure 25].
Operating  System Operating  System Operating  System
20KB Unused 20KB Unused 20KB Unused
In Use In Use In Use
10KB Unused 10KB Unused :
25KB Unused | Ret F258 Unused [esis  a=, fe ese
In Use In Use In Use
[Figure  25] Solving  Fragmentation  Problem  through  Coalescing  Technique
52 TOPCIT  ESSENCE


--- Page 54 ---
Overview  of System Architecture )
05 Scheduling
OS supporting  multiple  programming  uses scheduling  to allocate  processes  to the CPU efficiently,
A) Purpose  of scheduling
+ Fairness  to processes
+ Maximizing  throughput  per unit time
+ Minimizing  response  time
+ Predictable  execution  time
+ Preventing  system  overload
+ Balanced  resource  utilization
+ Preventing  indefinite  process  execution
+ Implementing  priority  schemes
B) Type and characteristics  of scheduling
<Table 8> shows the types and characteristics  of scheduling  algorithms.
+ Preemptive  scheduling:  A process  can take CPU resources  while another  process  is occupying  them.
+ Non-preemptive  scheduling:  If a CPU resource is allocated  to a process,  it cannot  be allocated  to another  process
until the task has been completed.
<Type 8) Types and Characteristics  of Scheduling  Algorithms
Type Method Characteristics Class
~ The simplest  scheduling  technique  to allocate  CPU
resources  according  to the order in which they are
FIFO. requested
(First In First Out) - Amethod  of allocating  CPUs in the order in which they
entered  the rows (queue  matrix)  of resource  requesting
processes- Not suitable  for interactive
- Simple and fair Non preemptive
- Prediction  of response  speed
- Fixed priority
- Variable  priority Non preemptive
- Purchased  priorityPriorit - Amethod  of assigning  the priority  to each process  and
y allocating  the CPU resources  to the highest priority
- Advantageous  to jobs with the short.
execution  time and a long time Non preemptive
expected  for large jobsSJF (Shortest  Job — - Amethod  of allocating  CPU resources  to the process  with
First) the shortest  expected  job time
- Amethod  of allocating  CPU resources  to the process  with - Job processing  is the same as SJF,
the shortest  expected  remaining  job time but it theoretically  requires  the least Non preemptive
- Can be considered  as preemptive  SJF waiting  time.SRT(Shortest.
Remaining  Time)
Like FIFO, it allocates  CPU resources  according  to the order
in which they are requested,  but each process canuse the Effective  for TSS
CPU only for the specified period It canbe considered  asa”  22” € 2S FIFO ifthe specified allocation
R-R(Round  Robin) y p P time is large Preemptive
preemptive  FIFO process.
The most suitable  method  for time series segmentation
(TSS)- Context  excha...
